---
engine: jupyter
---

# Executive Summary
---

**Problem**: Evaluating NFL kickers accurately mid-season presents a significant challenge due to the sparsity of game data and the varying contexts of individual field goal attempts. Traditional metrics often struggle to provide insights into true player skill when sample sizes are limited.

**Solution**: This report proposes a Bayesian hierarchical model designed to evaluate the performance of NFL kickers. The model accounts for both individual player skill and the non-linear relationship between kick distance and the probability of success. By estimating unique player effects and then applying these predictions to a standardized set of simulated kicks (25-60 yards), I derive a contextualized and stable evaluation metric.

**Key Findings**: The Simulated Field Goal Over Expected (FGOE) metric, generated by this model, offers a superior and more reliable assessment of kicker performance compared to traditional observed metrics. This approach successfully mitigates the noise introduced by small sample sizes, enabling better comparisons across players. For example, this analysis demonstrates how the Simulated FGOE accurately positions elite kickers like Justin Tucker at the top, even when their traditional FGOE is skewed by things such as blocked kicks.

**Impact**: This model provides a way to quantify kicker performance. It not only offers a clear "rating" metric, but also quantifies the uncertainty surrounding each player's estimated skill through credible intervals. This allows decision-makers to distinguish between genuinely high-performing kickers and those whose performance might be inflated by favorable circumstances or limited data. Players with fewer historical kicks will naturally show larger uncertainty bands, providing a realistic view of the confidence in their evaluation, while consistently strong performers will show tighter, higher-valued estimates. This data-driven insight empowers scouting teams and analysts to make more informed personnel decisions.

## Contents

- [The Model](#the-model)
- [Traditional FGOE](#traditional-fgoe)
- [Simulated FGOE](#simulated-fgoe)
- [Leaderboard](#leaderboard)


## The Model

::: {.callout-note}
Any code not directly related to the model or derived metrics, such as plotting functions or library imports, has been collapsed to improve readability. You can expand these code blocks by clicking on them.
:::

All necessary library imports and custom utility functions for data preparation, modeling, and plotting are defined below:

```{python}
# | code-fold: true
import bambi as bmb
import polars as pl
import numpy as np
import arviz as az
import polars as pl
import bambi as bmb
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from arviz.labels import BaseLabeller

from great_tables import GT, md, html

from plotnine import *


# This is from an LLM
class PlayerOnlyLabeller(BaseLabeller):
    def make_label_flat(self, var_name, coord_values=(), dims=(), index=None):
        # coord_values is a dict or list of tuples — try to extract the player name
        if isinstance(coord_values, dict):
            return coord_values.get("player_name", var_name)
        elif isinstance(coord_values, (list, tuple)) and coord_values:
            return coord_values[0][1]  # (dim_name, value)
        else:
            return var_name


def plot_fgoe(idata_fgoe, title, highlight=None, **kwargs):
    fgoe_mean = idata_fgoe["fgoe"].mean(("chain", "draw"))
    sorted_idata = idata_fgoe.sortby(fgoe_mean * -1)
    player_names = sorted_idata["player_name"].values.astype(str)

    axs = az.plot_forest(
        sorted_idata,
        var_names=["fgoe"],
        combined=True,
        labeller=PlayerOnlyLabeller(),
        **kwargs,
    )

    ax = axs[0]
    ax.axvline(x=0, color="black", linestyle="--", linewidth=1)
    ax.set_title(title)
    ax.set_xlabel("FGOE")
    ax.tick_params(axis="y", labelsize=10)

    # Highlight player(s)
    if highlight is not None:
        if isinstance(highlight, str):
            highlight = [highlight]
        highlight_set = set(highlight)

        # Y-axis tick labels are reversed from player_names
        yticks = ax.get_yticklabels()
        for i, label in enumerate(player_names[::-1]):
            if label in highlight_set:
                yticks[i].set_color("crimson")
                yticks[i].set_fontweight("bold")

    return ax


def prep_data() -> pl.DataFrame:
    """
    Create the base dataframe combining the two data sources

    Returns:
        pl.DataFrame: dataframe of field goal attempts with kicker info
    """
    df_field_goals = pl.read_csv("data/field_goal_attempts.csv")

    df_kickers = pl.read_csv("data/kickers.csv")

    df = (
        df_field_goals.join(df_kickers, on="player_id")
        .rename({"attempt_yards": "distance"})
        .with_columns(
            success=pl.when(pl.col("field_goal_result") == "Made")
            .then(pl.lit(1))
            .otherwise(pl.lit(0)),
            player_id=pl.col("player_id").cast(pl.Utf8).cast(pl.Categorical),
            player_name=pl.col("player_name").cast(pl.Categorical),
            season_type=pl.col("season_type").cast(pl.Categorical),
        )
    )

    return df


def create_grid(df: pd.DataFrame) -> pd.DataFrame:
    """
    Takes in a pandas dataframe and creates 1 row per kick attempt
    for each kicker in 2018

    Bambi relies on pandas...

    Returns:
        pd.DataFrame
    """
    df_2018 = df[df["season"] == 2018]

    # grid for kicks between 25 and 60 yards
    df_grid_18 = (
        df_2018[["player_id", "player_name"]]
        .drop_duplicates()
        .assign(key=1)
        .merge(
            pd.DataFrame({"distance": [x for x in range(25, 61)]}).assign(key=1),
            on="key",
        )
        .assign(__obs__=lambda x: np.arange(x.shape[0]))
    )

    return df_grid_18


def pointrange_data(
    df: pl.DataFrame | pd.DataFrame, field="fgoe", group=["player_id", "player_name"]
) -> pl.DataFrame:

    if isinstance(df, pd.DataFrame):
        df = pl.from_pandas(df)

    # Custom function to extract HDI bounds
    def hdi_bounds(values, prob=0.9):
        hdi_result = az.hdi(values.to_numpy(), hdi_prob=prob)
        return hdi_result[0], hdi_result[1]  # lower, upper

    def hdi_bounds_50(values):
        hdi_result = az.hdi(values.to_numpy(), hdi_prob=0.5)
        return hdi_result[0], hdi_result[1]  # lower, upper

    data = (
        df.group_by(group)
        .agg(
            fgoe=pl.col(field).mean(),
            hdi_90=pl.col(field).map_elements(
                lambda x: hdi_bounds(x, 0.9), return_dtype=pl.List(pl.Float64)
            ),
            hdi_50=pl.col(field).map_elements(
                lambda x: hdi_bounds_50(x), return_dtype=pl.List(pl.Float64)
            ),
        )
        .with_columns(
            ymin=pl.col("hdi_90").list.get(0),
            ymax=pl.col("hdi_90").list.get(1),
            hdi_lower=pl.col("hdi_50").list.get(0),
            hdi_upper=pl.col("hdi_50").list.get(1),
        )
        .drop(["hdi_90", "hdi_50"])
        .with_columns(
            highlight=pl.when(pl.col("player_name") == "JUSTIN TUCKER")
            .then(pl.lit("red"))
            #.when(pl.col("player_name") == "ROBERTO AGUAYO")
            #.then(pl.lit("blue"))
            .otherwise(pl.lit("grey"))
        )
        .sort("fgoe", descending=True)
    )

    return data


```

### Data

The data contains kicks for multiple seasons, where each kick event is a row.

```{python}

df = prep_data()
# Bambi currently expects Pandas DataFrames,
# so the Polars DataFrame is converted for model fitting.
df_model_pd = df.to_pandas()

```

There is clear evidence of a non-linear relationship between kick distance and success rate, as shown in the plot below. Using splines in the model will allow us to estimate this relationship much more realistically than a simple polynomial model.

```{python}
# | code-fold: true
df_plot = df.group_by("distance").agg(pl.col("success").sum() / pl.count("success"))
(
    ggplot(df_plot, aes("distance", "success"))
    + geom_smooth()
    + geom_point(alpha = 0.2)
    + labs(
        title="Success Rate by Distance - Observed Data",
        x="Attempt Distance (yards)",
        y="Success Rate",
    )
    + theme_bw()
)

```

Additionally, players have varying skill levels that traditional analytics cannot fully capture. The sparse data and hierarchical structure of the underlying process make this well-suited for hierarchical modeling approaches.

Using this domain knowledge, I will begin by building a simple model incorporating both kick distance (via splines) and individual player effects as primary features.

### Model Setup, Priors, & Justification

The following model was used:

$$
\begin{aligned}
y_i &\sim \operatorname{Bernoulli}(p_i) \\
\operatorname{logit}(p_i) &= \alpha_0 + \alpha_{j[i]} + \sum_{k=1}^K B_k(x_i) \, \beta_k \\
\alpha_0 &\sim \operatorname{Normal}(1, 1) \\ % Global fixed intercept
\alpha_j &\sim \operatorname{Student-t}(3, 0, \sigma_\alpha) \\
\sigma_\alpha &\sim \operatorname{HalfNormal}(0.5) \\
\beta_k &\sim \operatorname{Student-t}(3, 0, 1) \quad \text{for } k = 1, \dots, K \\
\end{aligned}
$$

- Bernoulli likelihood for the binary outcome
- Partially pooled intercepts by player
- The distribution of player effects share the same sigma parameter
- Spline with basis functions applied to distance

This model generates player-specific intercepts that adapt based on individual performance and data availability. Players with more observations will have their estimates pulled toward their own performance, while those with limited data will remain closer to the population prior. The shared variance across all players creates partial pooling, where information is borrowed between players to improve estimates for those with limited data.

### Priors

Student-t priors were chosen for their heavier tails, providing more robust estimates to handle outliers or less certain prior knowledge, particularly for individual player effects.

One of the strengths of Bayesian modeling is the ability to incorporate domain knowledge through prior distributions. For instance, we expect field goals from typical distances—such as 30 yards—to be successful more often than not. To reflect this, I placed a prior on the intercept of $Normal(1, 1)$, which corresponds to a probability greater than 50% (with the use of the logit link function).

```{python}
# spline specification, miss you mgcv::s()
s = "bs(distance, df=4, intercept=False)"
priors = {
    # global intercept, more likely for success
    "Intercept": bmb.Prior('Normal', mu = 1, sigma = 1),
    # priors on spline
    s: bmb.Prior("StudentT", nu=3, mu=0, sigma=1),
    # prior for player componenent, shared sigma parameter
    "1|player_id": bmb.Prior(
        "StudentT", nu=3, mu=0, sigma=bmb.Prior("HalfNormal", sigma=0.5)
    ),
}

# straightforward bambi definition
model = bmb.Model(
    f"success ~ (1|player_id) + {s}",
    data=df_model_pd,
    family="bernoulli",
    priors=priors,
    # these are default but critical to understand
    center_predictors=True, # better numerical stability
    noncentered=True # non-centered parameterization helps with hard to sample geometries
)

model

```

After specifying the model structure and priors, prior predictive checks serve as a diagnostic tool to assess whether the model's implied outcomes are reasonable before observing any data. In this case, since field goals are generally made more often than missed, the prior predictive distribution should reflect a higher probability of success. The results from the prior predictive check suggest that the priors lead to outcomes consistent with this expectation, indicating that the model is plausible data-generating process.

#### Prior distributions

```{python}
#| code-fold: true
model.build();
model.plot_priors();
```

#### Prior predictive checks

```{python}
#| code-fold: true
df_prior = pl.from_pandas(
    model.prior_predictive().prior_predictive["success"].to_dataframe().reset_index()
)

(
    ggplot(aes("success", "len"))
    + geom_col(
        aes(group="draw"),
        position="identity",
        fill=None,
        color="#00000013",
        alpha=0.01,
        data=df_prior.group_by(["draw", "success"]).len(),
    )
    + geom_col(
        fill=None,
        color="firebrick",
        size = 1.5,
        data=df.group_by(["success"]).len(),
    )
    + theme_bw()
    + labs(
        title ='Prior Predictive Checks',
        x = 'Success',
        y = 'Count'
    )
    + scale_x_continuous(breaks=[0, 1])
    + theme(
        panel_grid_minor=element_blank(),
        panel_grid_major_x=element_blank()
    )
)
```



#### Fitting the model

```{python}
idata = model.fit(draws=1000, chains=4, cores=1, random_seed=527)
az.plot_trace(idata, kind='rank_bars');
```

It's important to check diagnostics in Bayesian models. While this doesn't account for causality or domain-specific modeling checks, it ensures the MCMC algorithm is working as expected. All chains converged without divergent transitions, and effective sample sizes are adequate.

### Spline

The table below shows the summary of spline coefficient distributions.

```{python}
az.summary(idata, var_names=["Intercept", s])
```

Unlike player effect coefficients, they are not as easily interpretable in a table. The plots below are to help understand how they come together. First, plotting each basis function and it's new shape after learning parameter weights.

Each basis function with the model parameters applied to it.

```{python}
# | code-fold: true
idata.posterior[s].to_dataframe()

w = idata.posterior[s].mean(("chain", "draw")).values

spline_basis = pd.DataFrame(model.components["p"].design.common[s])

df_spline = (
    (spline_basis * w.T)
    .assign(distance=df_model_pd["distance"])
    .melt("distance", var_name="basis_idx", value_name="value")
    .assign(basis_idx=lambda x: x["basis_idx"].astype(str))
)  # .query('distance > 20')

(
    ggplot(df_spline, aes("distance", "value", color="factor(basis_idx)"))
    + geom_line(size = 1.2)
    + labs(title="Basis Function Shape", color="bf")
    + theme_bw()
    + theme(legend_position="top")
)
```

The plot below shows them aggregated together to produce the estimated effect of distance on FG success.

```{python}
# | code-fold: true
(
    ggplot(df_spline.groupby("distance", as_index=False)["value"].mean())
    + geom_line(aes("distance", "value"), size = 1.2)
    + labs(title="General Spline Shape")
    + theme_bw()
)
```

### Player Effects

The table above summarizes the posterior distributions for each player's individual effect (intercept) after accounting for kick distance.

```{python}
#| code-fold: true
az.summary(idata, var_names="1|player_id").reset_index().assign(
    player_id=lambda x: x["index"].str.extract(r"player_id\[(\d+)\]")
).merge(
    df_model_pd[["player_id", "player_name"]]
    .drop_duplicates()
    .assign(player_id=lambda x: x.player_id.astype(str))
).assign(
    index=lambda x: x.player_name
).drop(
    ["player_id", "player_name"], axis=1
).rename(
    columns={"index": "player_name"}
).sort_values('mean', ascending=False)
```

While these individual player effects are powerful for understanding the model's insights into each kicker's baseline skill, their direct interpretation as raw coefficients can be challenging to intuitively communicate to non-technical stakeholders as a standalone evaluation metric. This is why I will move towards the Simulated Field Goal Over Expected (FGOE), which translates these effects into a more relatable and actionable "rating" of expected performance on a standardized set of kicks.

## Field Goals Over Expected

### Traditional FGOE

What I outlined in my application was establishing at minimum a Field Goal Over Expected model. This contextualizes each kick and helps identify players over- or underperforming relative to expectation. This metric is used across multiple sports because it's intuitive and easily digestible, especially for audiences with less analytical training.

#### 2018 results

```{python}
df_2018 = df_model_pd[df_model_pd["season"] == 2018]

df_goe = model.predict(idata, data=df_2018, inplace=False)

# takes the posterior predictive distribution and subtracts the
# predicted probability from the observed kicks
df_fgoe = (
    df_goe["posterior"]["p"]
    .to_dataframe()
    .reset_index()
    .merge(df_2018.assign(__obs__=lambda x: np.arange(x.shape[0])), on="__obs__")
    .assign(fgoe=lambda x: x["success"] - x["p"])[
        ["chain", "draw", "fgoe", "player_name", "__obs__", "player_id"]
    ]
    .groupby(["chain", "draw", "player_name", "player_id"], observed=True)
    .agg({"fgoe": "sum"})
    .reset_index()
)
```

```{python}
#| code-fold: true
(
    ggplot(
        pointrange_data(df_fgoe),
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"),
        size=1.2,
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"), shape="o", fill="white")
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Field Goal Over Expected (FGOE)",
        subtitle="2018",
        y="Simulated FGOE",
    )
    + theme_bw()
    + theme(legend_position="none", axis_title_y=element_blank(), figure_size=(6, 6), dpi=100, panel_grid_minor=element_blank(), panel_grid_major_y=element_blank(), axis_text_y=element_text(size=6, weight='bold'))
    + geom_hline(yintercept=0, linetype="dashed")

)
```

Justin Tucker is the 3rd worst kicker? Objectively, Tucker was one of the best kickers in the league at the time of this data's shapshot. Pitching this model in 2018 would have been difficult, and rightfully so.

#### 2016-2017 results

If we look at the previous 2 seasons, Tucker was one of the better kickers in the by traditional FGOE.

```{python}
# | code-fold: true
df_before = df_model_pd[df_model_pd["season"].isin([2016, 2017])]

df_before = pointrange_data(
    (
        model.predict(idata, data=df_before, inplace=False)["posterior"]["p"]
        .to_dataframe()
        .reset_index()
        .merge(df_before.assign(__obs__=lambda x: np.arange(x.shape[0])), on="__obs__")
        .assign(fgoe=lambda x: x["success"] - x["p"])[
            ["chain", "draw", "fgoe", "player_name", "player_id", "__obs__"]
        ]
        .groupby(["chain", "draw", "player_name", "player_id"], observed=True)
        .agg({"fgoe": "sum"})
        .reset_index()
    )
)

(
    ggplot(
        df_before,
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"),
        size=1.2,
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"), shape="o", fill="white")
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Field Goal Over Expected (FGOE)",
        subtitle="2016 - 2017 | Justin Tucker has typically been one of the better kickers",
        y="Simulated FGOE",
    )
    + theme_bw()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 6),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=6, weight="bold"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
)
```


The problem with using this metric alone is that limited sample sizes can introduce a significant amount noise. Upon examining the dataset, there were multiple blocked field goals against Tucker. While outside the scope of this analysis, it's uncertain how much credit should be assigned to the kicker versus the blocking on the play for blocked kicks.

#### 2018 results - without blocks

When blocked kicks are removed, the model results align a _little_ better with expectations.

```{python}
# | code-fold: true
df_fgoe_no_block = (
    df_goe["posterior"]["p"]
    .to_dataframe()
    .reset_index()
    .merge(
        df_2018[~df_2018["field_goal_result"].isin(["Blocked"])].assign(
            __obs__=lambda x: np.arange(x.shape[0])
        ),
        on="__obs__",
    )
    .assign(fgoe=lambda x: x["success"] - x["p"])[
        ["chain", "draw", "fgoe", "player_name", "player_id", "__obs__"]
    ]
    .groupby(["chain", "draw", "player_name", "player_id"], observed=True)
    .agg({"fgoe": "sum"})
    .reset_index()
)

(
    ggplot(
        pointrange_data(df_fgoe_no_block),
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"),
        size=1.2,
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"), shape="o", fill="white")
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Field Goal Over Expected (FGOE)",
        subtitle="2018 | Removing blocked kicks",
        y="Simulated FGOE",
    )
    + theme_bw()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 6),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=6, weight="bold"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
)

```

However, that approach is arbitrary, a more robust solution is needed.

---

### Simulated FGOE

The model allows us to extract each player's intercept parameter after accounting for kick distance, and apply it to simulated data. This enables what-if scenarios and projections beyond only observed data. By modeling all kickers on the same set of kicks, we can establish expectations for what each kicker should make.

I chose linear intervals for kicks from 25-60 yards. This approach prioritizes simplicity — it's easy to explain and represents the first step toward gaining organizational buy-in for the model.

#### Standardized kick predictions

```{python}

df_grid_18 = create_grid(df_model_pd)

# Get posterior predictions without group effects
avg_preds = model.predict(
    idata, data=df_grid_18, inplace=False, include_group_specific=False
)

# posterior predictions on grid
preds = model.predict(idata, data=df_grid_18, inplace=False)

df_pred = preds.posterior["p"].to_dataframe().reset_index()
df_avg_pred = avg_preds["posterior"]["p"].to_dataframe().reset_index()

# take the predictions form the players and subtract the posterior predictions without group effects. This provides the main source of the metric for simulated FGOE
df_sfgoe = (
    df_pred.merge(df_avg_pred, on=["chain", "draw", "__obs__"], how="inner")
    .merge(
        df_grid_18[["player_id", "player_name", "distance", "__obs__"]],
        on=["__obs__"],
        how="left",
    )
    .assign(fgoe=lambda x: x["p_x"] - x["p_y"]) # players expectation - avg
)

# polars for efficient aggregation
df_output = (
    pl.from_pandas(df_sfgoe)
    .group_by(["player_id", "player_name", "draw", "chain"])
    .agg(pl.col("fgoe").sum())
)

df_output.group_by(['player_name', 'player_id']).agg(fgoe_mean = pl.col('fgoe').mean(), simulations = pl.len())
```

With the simulation of kicks, we can now look at a **Simulated FGOE**. This operates at the scale of posterior sample draws, with each player having 4,000 simulations across a distribution of kicks. This approach provides the ability to quantify uncertainty within the estimates and examine the range of potential outcomes, while addressing the sample size problem of a traditional FGOE.

#### Plotting Simulated FGOE

The plot below shows the posterior distributions of simulated FGOE. Compared to the traditional version, this metric is more stable and less noisy, and it follows intuitive principles that make interpretation easier. Many players cluster around zero, roughly representing replacement level, while deviations in either direction are easy to interpret.

For example, saying that Justin Tucker’s FGOE is between 1 and 4 field goals over expected is far more interpretable than stating that _the log-odds of the player’s intercept increases by 0.2 and 0.8_.

```{python}
# | code-fold: true
(
    ggplot(
        pointrange_data(df_output),
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"))
    + geom_linerange(aes(ymin="hdi_lower", ymax="hdi_upper"), size=1.5)
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Simulated Field Goal Over Expected (FGOE)",
        subtitle="2018",
        y="Simulated FGOE",
    )
    + theme_bw()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 6),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=6, weight="bold"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
)
```

#### Comparing output

To compare the traditional vs simulated FGOE, we can look at the posterior distributions between the two approaches.

::: {.callout-note}
The models aren't necesarily 1:1 comparisons of the same distribution of kicks, because in the traditional version we are limited to the kicks that the kicker attempted. What is more important is to look directionally at what the simulated model does compared to the traditional. Easily shown is Justin Tucker is firmly at the top of the list, without having to adjust arbitrarily for blocked kicks.
:::



```{python}
# | code-fold: true

df_comparison = pl.concat(
    [
        df_output.with_columns(model=pl.lit("Simulated")),
        pl.from_pandas(df_fgoe)
        .with_columns(model=pl.lit("Traditional"))
        .select(["player_id", "player_name", "draw", "chain", "fgoe", "model"]),
    ]
)

df_comparison_r = pointrange_data(
    df_comparison, group=["player_id", "player_name", "model"]
).join(
    df_output.group_by(["player_name", "player_id"]).agg(
        plot_sort=pl.col("fgoe").mean(), simulations=pl.len()
    ),
    on=["player_id", "player_name"],
)

(
    ggplot(
        df_comparison_r, aes("reorder(player_name, plot_sort)", "fgoe", color="model")
    )
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"), size=1.2, position=position_dodge(0.5)
    )
    + geom_pointrange(
        aes(ymin="ymin", ymax="ymax"),
        shape="o",
        fill="white",
        position=position_dodge(0.5),
    )
    + coord_flip()
    + scale_color_manual(values=["grey","black"])
    + labs(
        y="Simulated FGOE",
    )
    + theme_bw()
    + theme(
        legend_position="top",
        axis_title_y=element_blank(),
        figure_size=(7, 7),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=6, weight="bold"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
    + labs(
        title="Simulated Field Goal Over Expected (FGOE)",
        y="Simulated FGOE",
        color="Model:",
    )
)
```


#### Which player would we prefer?

Now, even if a kicker hasn't attempted a 60 yard kick, we can provide an estimate for what that might look like.

```{python}
# | code-fold: true
plot_df = pl.from_pandas(
    df_sfgoe[df_sfgoe["player_name"].isin(["JUSTIN TUCKER", "ZANE GONZALEZ"])].assign(
        player_name=lambda x: x["player_name"].astype(str)
    )
)

pointrange_data(plot_df, field="p_x", group=["player_name", "distance"])

(
    ggplot(aes("distance", "p_x", color="player_name"))
    + geom_ribbon(
        aes(y="fgoe", ymin="ymin", ymax="ymax", fill="player_name"),
        # fill="#00000033",
        data=pointrange_data(plot_df, field="p_x", group=["player_name", "distance"]),
        alpha=0.2,
        outline_type=element_blank(),
        show_legend={"fill": False},
    )
    + geom_ribbon(
        aes(y="fgoe", ymin="hdi_lower", ymax="hdi_upper", fill="player_name"),
        # fill="#00000053",
        data=pointrange_data(plot_df, field="p_x", group=["player_name", "distance"]),
        alpha=0.4,
        outline_type=element_blank(),
        show_legend={"fill": False},
    )
    + geom_line(
        data=plot_df.group_by(["player_name", "distance"]).agg(pl.col("p_x").mean()),
        size=1.5,
    )
    + theme_bw()
    + theme(legend_position="top", panel_grid_minor=element_blank())
    + labs(
        title="Modeled Probability by Distance",
        y="P(Make)",
        x="Distance",
        color="Player:",
    )
    + scale_x_continuous(limits=(25, 60))
    + scale_color_manual(values=("midnightblue", "firebrick"))
    + scale_fill_manual(values=("midnightblue", "firebrick"))
)
```

## Leaderboard

The table below presents the generated leaderboard, showcasing the player_id, player_name, rating (Simulated FGOE), and rank for the top 10 and bottom 10 kickers evaluated by the model. The rank column is in ascending order, where a lower rank indicates a better performance.

```{python}
# | code-fold: true


def create_leaderboard(df: pl.DataFrame) -> pl.DataFrame:
    """
    Create the leaderboard of FGOE values

    Args:
        df: (pl.DataFrame) dataframe from `create_fgoe_draws()`

    Returns:
        pl.DataFrame: Polars dataframe of leaderboard

    """
    idata_post = df.to_pandas().set_index(["chain", "draw", "player_name"]).to_xarray()

    # perc player was in top 5 of a draw
    df_top_5 = (
        df.with_columns(
            player_name=pl.col("player_name").cast(pl.Utf8),
            rank=pl.col("fgoe").rank("dense", descending=True).over(["draw", "chain"]),
        )
        .group_by(["player_name"])
        .agg(top_5=(pl.col("rank") <= 5).mean())
        .sort("top_5", descending=True)
    )

    # summary table of the metric, with rank added and joined with top 5
    leaderboard = (
        (
            pl.from_pandas(
                az.summary(idata_post, var_names=["fgoe"], hdi_prob=0.9)
                .reset_index()
                .assign(player_name=lambda x: x["index"].str.extract(r"fgoe\[(.+)\]"))
            )
            .join(df_top_5, on="player_name")
            .sort(["mean"], descending=True)
            .with_row_index("rank", offset=1)
            .rename({"mean": "rating"})
            .join(
                df[["player_name", "player_id"]]
                .unique()
                .with_columns(pl.col("player_name").cast(pl.Utf8)),
                on="player_name",
            )
        )
        .select(
            [
                "player_id",
                "player_name",
                "rank",
                "rating",
                "sd",
                "hdi_5%",
                "hdi_95%",
                "top_5",
            ]
        )
        .sort(["rank"])
    )

    return leaderboard


leaderboard = create_leaderboard(df_output)
leaderboard.write_csv("leaderboard.csv")

(
    GT(
        pl.concat([leaderboard.head(10), leaderboard.tail(10)]).rename(
            {"hdi_5%": "lower", "hdi_95%": "upper"}
        )
    )
    .tab_header(
        title="2018 NFL Kicker Leaderboard", subtitle="Top and bottom 10 players shown"
    )
    .tab_spanner(label="Simulated FGOE", columns=["rating", "sd", "lower", "upper"])
    # .tab_stubhead(label="rank")
    .fmt_number(columns=["rating", "sd", "lower", "upper"], decimals=2)
    .fmt_percent("top_5", decimals=1)
    .data_color(
        columns=["rating"],
        domain=[leaderboard["rating"].min(), leaderboard["rating"].max()],
        palette=["rebeccapurple", "white", "green"],
        na_color="white",
    )
    .data_color(
        columns=["lower"],
        domain=[leaderboard["hdi_5%"].min(), leaderboard["hdi_5%"].max()],
        palette=["rebeccapurple", "white", "green"],
        na_color="white",
    )
    .data_color(
        columns=["upper"],
        domain=[leaderboard["hdi_95%"].min(), leaderboard["hdi_95%"].max()],
        palette=["rebeccapurple", "white", "green"],
        na_color="white",
    )
    .data_color(
        columns=["top_5"],
        domain=[0, 1],
        palette=["white", "green"],
        na_color="white",
    )
    .cols_label(
        player_id=html("Player Id"),
        player_name=html("Player Name"),
        rank="Rank",
        rating="Rating",
        sd="Variance",
        lower="Low-end",
        upper="High-end",
        top_5="Top 5",
    )
)

```

- **Justin Tucker** consistently ranks at the top of the leaderboard, demonstrating a high likelihood of being among the top 5 kickers by this metric. This positioning, even in seasons with a lot of variance (like 2018's blocked kicks), underscores the model's ability to provide a more stable and accurate assessment of true skill.
- **Robert Aguayo's** ranking reflects the influence of partial pooling from his prior seasons, contributing to his lower-end position despite a potentially stronger 2018 performance.
- **Tyler Davis** with only a single kick in the dataset, has a very wide credible interval. This indicates that while the model provides a point estimate, there is substantial uncertainty, suggesting his true skill could credibly range from one of the worst kickers to a top 5 performer. This demonstrates the model's ability to quantify uncertainty, allowing decision-makers to understand the confidence level in each player's evaluation, especially for those with sparse data.
