---
engine: jupyter
---

# Executive Summary
---

**Problem**: Evaluating NFL kickers accurately presents a significant challange due to the low sample sizes and varying contexts of field goal attempts, and is made even more challenging to do in-season. Tradditional metrics often struggle to provide insights into true player skill when sample sizes are limited.

**Solution**: This is a proposed Bayesian hierarchical model designed to evaluate the performance of NFL kickers. The model accounts for both individual player skill, and the non-linear relationship between kick distance and the probability of success. By estimating unique player effects and then applying these predictions to a standardized set of simulated kicks (25-60 yards), I derive a contextualized and stable evaluation metric.

**Key Findings**: The Simulated Field Goal Over Expected (FGOE) metric, generated by this model, offers a superior and more reliable assessment of kicker performance compared to traditional observed metrics. This approach successfully mitigates the noise introduced by small sample sizes, enabling better comparisons across players. For example, this analysis demonstrates how the Simulated FGOE accurately positions elite kickers like Justin Tucker at the top, even when their traditional FGOE is skewed by things such as blocked kicks.

**Impact**: This model provides a way to quantify kicker performance. It not only offers a clear "rating" metric, but also quantifies the uncertainty surrounding each player's estimated skill through credible intervals. This allows decision-makers to distinguish between genuinely high-performing kickers and those whose performance might be inflated by favorable circumstances or limited data. Players with fewer historical kicks will naturally show larger uncertainty bands, providing a realistic view of the confidence in their evaluation, while consistently strong performers will show tighter, higher-valued estimates. These insights empower scouting teams and analysts to make more informed player personnel decisions.

## Contents

- [The Model](#the-model)
- [Traditional FGOE](#traditional-fgoe)
- [Simulated FGOE](#simulated-fgoe)
- [Leaderboard](#leaderboard)


## The Model

::: {.callout-note}
Any code not directly related to the model or derived metrics, such as plotting functions or library imports, have been collapsed to improve readability. You can expand these code blocks by clicking on them.
:::

All necessary library imports and custom utility functions for data preparation, modeling, and plotting are defined below:

```{python}
# | code-fold: true
import bambi as bmb
import polars as pl
import numpy as np
import arviz as az
import polars as pl
import bambi as bmb
import pandas as pd

from great_tables import GT, md, html

# ggplot and an ad-hoc report is the only time I would do this
from plotnine import *


def prep_data() -> pl.DataFrame:
    """
    Create the base dataframe combining the two data sources

    Returns:
        pl.DataFrame: dataframe of field goal attempts with kicker info
    """
    df_field_goals = pl.read_csv("data/field_goal_attempts.csv")

    df_kickers = pl.read_csv("data/kickers.csv")

    df = (
        df_field_goals.join(df_kickers, on="player_id")
        .rename({"attempt_yards": "distance"})
        .with_columns(
            success=pl.when(pl.col("field_goal_result") == "Made")
            .then(pl.lit(1))
            .otherwise(pl.lit(0)),
            player_id=pl.col("player_id").cast(pl.Utf8).cast(pl.Categorical),
            player_name=pl.col("player_name").cast(pl.Categorical),
            season_type=pl.col("season_type").cast(pl.Categorical),
        )
    )

    return df


def create_grid(df: pd.DataFrame) -> pd.DataFrame:
    """
    Takes in a pandas dataframe and creates 1 row per kick attempt
    for each kicker in 2018

    Bambi relies on pandas...

    Returns:
        pd.DataFrame
    """
    df_2018 = df[df["season"] == 2018]

    # grid for kicks between 25 and 60 yards
    df_grid_18 = (
        df_2018[["player_id", "player_name"]]
        .drop_duplicates()
        .assign(key=1)
        .merge(
            pd.DataFrame({"distance": [x for x in range(25, 61)]}).assign(key=1),
            on="key",
        )
        .assign(__obs__=lambda x: np.arange(x.shape[0]))
    )

    return df_grid_18


palette = ["#001e42", "#535c78", "#9fa3b3", "#f1f1f1", "#ffbea4", "#ff895a", "#ff4b00"]


def pointrange_data(
    df: pl.DataFrame | pd.DataFrame,
    field: str = "fgoe",
    group: list = ["player_id", "player_name"],
) -> pl.DataFrame:
    """
    Returns `arviz.hdi()` values from a long dataframe. These values are used to
    produce high density credible intervals for plots.

    Args:
        df: dataframe that contains multiple records per group
        field: (str, optional) field to aggreagate hdci on
        group: (list, optional) fields to group aggregations by

    Returns:
        pl.DataFrame: dataframe with 1 row per group and hdci intervals
    """

    if isinstance(df, pd.DataFrame):
        df = pl.from_pandas(df)

    # Custom function to extract HDI bounds
    def hdi_bounds(values, prob=0.9):
        hdi_result = az.hdi(values.to_numpy(), hdi_prob=prob)
        return hdi_result[0], hdi_result[1]  # lower, upper

    def hdi_bounds_50(values):
        hdi_result = az.hdi(values.to_numpy(), hdi_prob=0.5)
        return hdi_result[0], hdi_result[1]  # lower, upper

    data = (
        df.group_by(group)
        .agg(
            fgoe=pl.col(field).mean(),
            hdi_90=pl.col(field).map_elements(
                lambda x: hdi_bounds(x, 0.9), return_dtype=pl.List(pl.Float64)
            ),
            hdi_50=pl.col(field).map_elements(
                lambda x: hdi_bounds_50(x), return_dtype=pl.List(pl.Float64)
            ),
        )
        .with_columns(
            ymin=pl.col("hdi_90").list.get(0),
            ymax=pl.col("hdi_90").list.get(1),
            hdi_lower=pl.col("hdi_50").list.get(0),
            hdi_upper=pl.col("hdi_50").list.get(1),
        )
        .drop(["hdi_90", "hdi_50"])
        .with_columns(
            highlight=pl.when(pl.col("player_name") == "JUSTIN TUCKER").then(
                pl.lit(palette[-1])
            )
            # .when(pl.col("player_name") == "ROBERTO AGUAYO")
            # .then(pl.lit("blue"))
            .otherwise(pl.lit("grey"))
        )
        .sort("fgoe", descending=True)
    )

    return data


def title_name(df: pl.DataFrame | pd.DataFrame):
    """
    Title case names, primarily for plots.
    """
    if isinstance(df, pl.DataFrame):
        return df.with_columns(pl.col("player_name").cast(pl.Utf8).str.to_titlecase())
    elif isinstance(df, pd.DataFrame):
        return df.assign(player_name=lambda x: x.player_name.str.title())
    else:
        raise ValueError


def get_jt(df):
    shape = df.shape[0]
    var = df.with_row_index().filter(pl.col("player_name") == "Justin Tucker")['index'][0]

    return shape - var
```

### Data

The data contains kicks for multiple seasons, where each kick event is a row.

```{python}

df = prep_data()
# Bambi currently expects Pandas DataFrames,
# so the Polars DataFrame is converted for model fitting.
df_model_pd = df.to_pandas()

```

There is clear evidence of a non-linear relationship between kick distance and success rate, as shown in the plot below. Using splines in the model will allow us to estimate this relationship much more realistically than a quadratic model.

```{python}
#| code-fold: true
#| warning: false
df_plot = df.group_by("distance").agg(pl.col("success").sum() / pl.count("success"))
(
    ggplot(df_plot, aes("distance", "success"))
    + geom_smooth(color = palette[0])
    + geom_point(alpha = 0.2, color=palette[0])
    + labs(
        title="Success Rate by Distance - Observed Data",
        x="Attempt Distance (yards)",
        y="Success Rate",
    )
    + theme_minimal()
)

```

Additionally, players have varying skill levels that traditional analytics cannot fully capture. The sparse data and hierarchical structure of the underlying process makes this well-suited for hierarchical modeling methods.

I will begin by building a simple model incorporating both kick distance (via splines) and individual player effects as primary features.

### Model Setup, Priors, & Justification

The following model was used:

$$
\begin{aligned}
y_i &\sim \operatorname{Bernoulli}(p_i) \\
\operatorname{logit}(p_i) &= \alpha_0 + \alpha_{j[i]} + \sum_{k=1}^K B_k(x_i) \, \beta_k \\
\alpha_0 &\sim \operatorname{Normal}(1, 1) \\ % Global fixed intercept
\alpha_j &\sim \operatorname{Student-t}(3, 0, \sigma_\alpha) \\
\sigma_\alpha &\sim \operatorname{HalfNormal}(0.5) \\
\beta_k &\sim \operatorname{Student-t}(3, 0, 1) \quad \text{for } k = 1, \dots, K \\
\end{aligned}
$$

- Bernoulli likelihood for the binary outcome
- Partially pooled intercepts by player
- The distribution of player effects share the same sigma parameter
- Spline with basis functions applied to distance

This model generates player-specific intercepts that adapt based on individual performance and data availability. Players with more observations will have their estimates pulled toward their own performance, while those with limited data will remain closer to the population prior. The shared variance across all players creates partial pooling, where information is borrowed between players to improve estimates for those with limited data.

### Priors

Student-t priors were chosen for their heavier tails, providing more robust estimates to handle outliers or less certain prior knowledge. This is particularly useful for individual player effects.

One of the strengths of Bayesian modeling is the ability to incorporate domain knowledge through prior distributions. For instance, we expect field goals from typical distances — such as 30 yards — to be successful more often than not. To reflect this, I placed a prior on the intercept of $Normal(1, 1)$, which corresponds to a probability greater than 50% (with the use of the logit link function).

```{python}
# spline specification, miss you mgcv::s()
s = "bs(distance, df=4, intercept=False)"
priors = {
    # global intercept, more likely for success
    "Intercept": bmb.Prior('Normal', mu = 1, sigma = 1),
    # priors on spline
    s: bmb.Prior("StudentT", nu=3, mu=0, sigma=1),
    # prior for player componenent, shared sigma parameter
    # sigma is they hyperprior paramter shared
    "1|player_id": bmb.Prior(
        "StudentT", nu=3, mu=0, sigma=bmb.Prior("HalfNormal", sigma=0.5)
    ),
}

# bambi definition
model = bmb.Model(
    f"success ~ (1|player_id) + {s}",
    data=df_model_pd,
    family="bernoulli",
    priors=priors,
    # default but critical
    center_predictors=True, # better numerical stability
    noncentered=True, # non-centered parameterization helps with hard to sample geometries
)

model

```


After specifying the model structure and priors, a diagnostic tool in prior predictive checks help assess whether the model generates realistic values. In this case, since field goals are made more often than missed, the prior predictive distribution should reflect more success.

#### Prior distributions

```{python}
#| code-fold: true
model.build();
model.plot_priors(random_seed=527);
```

#### Prior predictive checks

```{python}
# | code-fold: true
df_prior = pl.from_pandas(
    model.prior_predictive().prior_predictive["success"].to_dataframe().reset_index()
)

(
    ggplot(aes("success", "len"))
    + geom_col(
        aes(group="draw", color="t"),
        position="identity",
        fill=None,
        alpha=0.01,
        data=(
            df_prior.group_by(["draw", "success"])
            .len()
            .with_columns(t=pl.lit("prior predictive"))
        ),
    )
    + geom_col(
        aes(color="t"),
        fill=None,
        size=1.5,
        data=(
            df_prior.group_by(["draw", "success"])
            .len()
            .group_by("success")
            .agg(pl.col("len").mean())
            .with_columns(t=pl.lit("prior predictive mean"))
        ),
    )
    + theme_minimal()
    + labs(title="Prior Predictive Checks", x="Success", y="Count", color="")
    + scale_x_continuous(breaks=[0, 1])
    + theme(
        panel_grid_minor=element_blank(),
        panel_grid_major_x=element_blank(),
        legend_position=(0.1, 1),
        legend_background=element_blank(),
    )
    + scale_color_manual(values=(palette[2], palette[-1]))
)

```

The results from the prior predictive check suggest that the priors lead to outcomes consistent with the expectation, indicating that the model has a plausible data-generating process.

#### Fitting the model

The model fits by sampling the posterior distribution of the paramters.

```{python}
idata = model.fit(draws=1000, chains=4, cores=1, random_seed=527)
az.plot_trace(idata, kind='rank_bars');
```

Another diagnostic tool in Bayesian models is to check the Markov Chain Monte Carlo (MCMC) algorithm. While these diagnostics don't account for causality or domain-specific modeling checks, it ensures the MCMC algorithm is working as expected. All chains converged without divergent transitions, and effective sample sizes are adequate.


### Player Effects

The table below summarizes the posterior distributions for each player's individual effect (intercept) after accounting for kick distance.

```{python}
#| code-fold: true
az.summary(idata, var_names="1|player_id").reset_index().assign(
    player_id=lambda x: x["index"].str.extract(r"player_id\[(\d+)\]")
).merge(
    df_model_pd[["player_id", "player_name"]]
    .drop_duplicates()
    .assign(player_id=lambda x: x.player_id.astype(str))
).assign(
    index=lambda x: x.player_name
).drop(
    ["player_id", "player_name"], axis=1
).rename(
    columns={"index": "player_name"}
).sort_values('mean', ascending=False)
```

While these individual player effects help understand the model's insights into each kicker's baseline skill, their direct interpretation as raw coefficients can be challenging to communicate to non-technical stakeholders as an intuitive standalone evaluation metric. This is why I will move towards the Simulated Field Goal Over Expected (FGOE), which translates these effects into a more relatable and actionable "rating" of expected performance on a standardized set of kicks.

### Spline

Splines togehter with field goal distance were used, and the following discusses how those work with the model. The table below shows the summary of spline coefficient distributions.

```{python}
az.summary(idata, var_names=["Intercept", s])
```

Unlike player effect coefficients, they are not as easily interpretable in a table. The plots below are to help build an understanding of how they come together. There are 4 basis functions used in the model. Below is a plot of each basis function and it's new shape after learning parameter weights.

#### Basis functions

```{python}
# | code-fold: true
idata.posterior[s].to_dataframe()

w = idata.posterior[s].mean(("chain", "draw")).values

spline_basis = pd.DataFrame(model.components["p"].design.common[s])

df_spline = (
    (spline_basis * w.T)
    .assign(distance=df_model_pd["distance"])
    .melt("distance", var_name="basis_idx", value_name="value")
    .assign(basis_idx=lambda x: x["basis_idx"].astype(str))
)  # .query('distance > 20')

(
    ggplot(df_spline, aes("distance", "value", color="factor(basis_idx)"))
    + geom_line(size = 1.2)
    + labs(title="Basis Function Shape", color="bf")
    + theme_minimal()
    + theme(legend_position="top")
    + labs(x = 'Distance', y = 'Value')
)
```

#### Aggregated spline

Finally, they can be aggregated together to produce the estimated effect of distance on FG success.

```{python}
# | code-fold: true
(
    ggplot(df_spline.groupby("distance", as_index=False)["value"].mean())
    + geom_line(aes("distance", "value"), size=1.2, color=palette[0])
    + labs(title="General Spline Shape")
    + theme_minimal()
    + labs(x="Distance", y="Value")
)
```

## Field Goals Over Expected

### Traditional FGOE

What I outlined in my application was establishing at minimum a Field Goal Over Expected model. This contextualizes each kick and helps identify players over or underperforming relative to expectation. This metric is used across multiple sports because it's intuitive and easily digestible, especially for audiences with less analytical training. It does a better job than raw completion rate, but has room for improvement.

#### 2018 results

To calculate a traditional FGOE, the average expected probabilities accounting for distance and removing the player grouping effects can be used. FGOE is simply the observed event minus those expected probabilites. This provides a distribution of what each players traditional FGOE metric looks like.

```{python}
df_2018 = df_model_pd[df_model_pd["season"] == 2018]

df_tfgoe = model.predict(
    idata, data=df_2018, inplace=False, include_group_specific=False
)

df_preds_18 = model.predict(
    idata, data=df_2018, inplace=False, include_group_specific=False
)

# takes the posterior predictive distribution and subtracts the
# predicted probability from the observed kicks
df_fgoe = (
    df_preds_18["posterior"]["p"]
    .to_dataframe()
    .reset_index()
    .merge(df_2018.assign(__obs__=lambda x: np.arange(x.shape[0])), on="__obs__")
    .assign(fgoe=lambda x: x["success"] - x["p"])[
        ["chain", "draw", "fgoe", "player_name", "__obs__", "player_id"]
    ]
    .groupby(["chain", "draw", "player_name", "player_id"], observed=True)
    .agg({"fgoe": "sum"})
    .reset_index()
)
```

```{python}
# | code-fold: true
p1 = title_name(pointrange_data(df_fgoe))
(
    ggplot(
        title_name(pointrange_data(df_fgoe)),
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_hline(yintercept=0, linetype="solid")
    #+ geom_vline(xintercept=36, linetype="dotted", color="#ff895a")
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"),
        size=1.2,
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"), shape="o", fill="white")
    + coord_flip()
    + scale_color_identity()
    # scale_color_manual(values=("#9fa3b3", "#ff4b00"))
    + labs(
        title="Field Goal Over Expected (FGOE)",
        subtitle="2018",
        y="Traditional FGOE",
    )
    + theme_minimal()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 8),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=8),
    )
     + geom_vline(xintercept=get_jt(p1), linetype="dotted", color="#ff895a")
)
```

One problem... Justin Tucker is replacement-level kicker? Objectively, Tucker was one of the best kickers in the league at the time of the data. It wouldn't be unlike presenting a QB model with Patrick Mahomes or Tom Brady at the bottom. Pitching this metric and leaderboard in 2018 would have been difficult, and rightfully so.

#### 2016-2017 results

Looking at the previous 2 seasons, Tucker was one of the best kickers in the league by traditional FGOE.

```{python}
# | code-fold: true
df_before = df_model_pd[df_model_pd["season"].isin([2016, 2017])]

df_before = pointrange_data(
    (
        model.predict(
            idata, data=df_before, inplace=False, include_group_specific=False
        )["posterior"]["p"]
        .to_dataframe()
        .reset_index()
        .merge(df_before.assign(__obs__=lambda x: np.arange(x.shape[0])), on="__obs__")
        .assign(fgoe=lambda x: x["success"] - x["p"])[
            ["chain", "draw", "fgoe", "player_name", "player_id", "__obs__"]
        ]
        .groupby(["chain", "draw", "player_name", "player_id"], observed=True)
        .agg({"fgoe": "sum"})
        .reset_index()
    )
)

df_before = title_name(df_before)
(
    ggplot(
        df_before,
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"),
        size=1.2,
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"), shape="o", fill="white")
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Field Goal Over Expected (FGOE)",
        subtitle="2016 - 2017 | Justin Tucker has been one of the best",
        y="Traditional FGOE",
    )
    + theme_minimal()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 8),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=8),
    )
    + geom_vline(xintercept=get_jt(df_before), linetype="dotted", color="#ff895a")
)
```

It isn't impossible, but it should be unlikely that he dropped from one of the best to one of the worst in 6 weeks.

The problem with using this metric alone is that limited sample sizes can introduce a significant amount noise. Upon examining the dataset, there were multiple blocked field goals against Tucker. While outside the scope of this analysis, it's uncertain how much credit should be assigned to the kicker versus the blocking on the play for blocked kicks.

#### 2018 results - without blocks

When blocked kicks are removed, the model results align a _little_ better with expectations.

```{python}
# | code-fold: true
df_fgoe_no_block = (
    df_preds_18["posterior"]["p"]
    .to_dataframe()
    .reset_index()
    .merge(
        df_2018[~df_2018["field_goal_result"].isin(["Blocked"])].assign(
            __obs__=lambda x: np.arange(x.shape[0])
        ),
        on="__obs__",
    )
    .assign(fgoe=lambda x: x["success"] - x["p"])[
        ["chain", "draw", "fgoe", "player_name", "player_id", "__obs__"]
    ]
    .groupby(["chain", "draw", "player_name", "player_id"], observed=True)
    .agg({"fgoe": "sum"})
    .reset_index()
)

p2 = title_name(pointrange_data(df_fgoe_no_block))
(
    ggplot(
        p2,
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"),
        size=1.2,
    )
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"), shape="o", fill="white")
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Field Goal Over Expected (FGOE)",
        subtitle="2018 | Removing blocked kicks",
        y="Traditional FGOE",
    )
    + theme_minimal()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 8),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=8),
    )
    + geom_vline(xintercept=get_jt(p2), linetype="dotted", color="#ff895a")
)

```

However, this approach is arbitrary as it requires manual intervention and doesn't provide a way to account for such events within the model itself, highlighting the need for a better solution.
---

### Simulated FGOE

The model allows us to extract each player's parameter distributions as well as distance effects, and apply it to simulated data. This enables what-if scenarios and projections beyond only observed data. By modeling all kickers on the same set of kicks, we can establish expectations for what each kicker could make in this hypothetical equal oppurtunity world.

I chose linear intervals for kicks from 25-60 yards. This approach prioritizes simplicity, and it's easy to explain. This represents the first step toward gaining organizational buy-in for the leaderboard.

#### Standardized kick predictions

```{python}

df_grid_18 = create_grid(df_model_pd)

# Get posterior predictions without group effects
avg_preds = model.predict(
    idata, data=df_grid_18, inplace=False, include_group_specific=False
)

# posterior predictions on grid
preds = model.predict(idata, data=df_grid_18, inplace=False)

df_pred = preds.posterior["p"].to_dataframe().reset_index()
df_avg_pred = avg_preds["posterior"]["p"].to_dataframe().reset_index()

# take the predictions form the players and subtract the posterior predictions without group effects. This provides the main source of the metric for simulated FGOE
df_sfgoe = (
    df_pred.merge(df_avg_pred, on=["chain", "draw", "__obs__"], how="inner")
    .merge(
        df_grid_18[["player_id", "player_name", "distance", "__obs__"]],
        on=["__obs__"],
        how="left",
    )
    .assign(fgoe=lambda x: x["p_x"] - x["p_y"])  # players expectation - avg
)

# polars for efficient aggregation
df_output = (
    pl.from_pandas(df_sfgoe)
    .group_by(["player_id", "player_name", "draw", "chain"])
    .agg(pl.col("fgoe").sum())
)

df_output.group_by(["player_name", "player_id"]).agg(
    fgoe_mean=pl.col("fgoe").mean(), simulations=pl.len()
)
```

With the simulation of kicks, we can now look at a **Simulated FGOE**. This operates at the scale of posterior sample draws, with each player having 4,000 simulations across a distribution of kicks. This approach provides the ability to quantify uncertainty within the estimates and examine the range of potential outcomes, while addressing the sample size problem of a traditional FGOE.

#### Plotting Simulated FGOE

The plot below shows the posterior distributions of simulated FGOE. Compared to the traditional version, this metric is more stable and less noisy, and it follows intuitive principles that make interpretation easier. Many players cluster around zero, roughly representing replacement level, while deviations in either direction are easy to interpret.

For example, saying that Justin Tucker’s FGOE is between 1 and 4 field goals over expected is far more interpretable than stating that _the log-odds of the player’s intercept increases by a value between 0.2 and 0.8_.

```{python}
# | code-fold: true
p3 = title_name(pointrange_data(df_output))
(
    ggplot(
        p3,
        aes("reorder(player_name, fgoe)", "fgoe", color="highlight"),
    )
    + geom_hline(yintercept=0, linetype="dashed")
    + geom_pointrange(aes(ymin="ymin", ymax="ymax"))
    + geom_linerange(aes(ymin="hdi_lower", ymax="hdi_upper"), size=1.5)
    + coord_flip()
    + scale_color_identity()
    + labs(
        title="Simulated Field Goal Over Expected (FGOE)",
        subtitle="2018",
        y="Simulated FGOE",
    )
    + theme_minimal()
    + theme(
        legend_position="none",
        axis_title_y=element_blank(),
        figure_size=(6, 8),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=8),
    )
    + geom_vline(xintercept=get_jt(p3), linetype="dotted", color="#ff895a")
)
```

#### Comparing output

To compare the traditional vs simulated FGOE, we can look at the posterior distributions between the two approaches.

::: {.callout-note}
The models aren't necesarily 1:1 comparisons of the same distribution of kicks, because with the traditional FGOE we are limited to the kicks that the kicker attempted. Simulated FGOE is applied to the same set of kicks for each kicker. It is more important to look directionally at each model. Easily shown is Justin Tucker is firmly at the top of the list, without having to adjust arbitrarily for blocked kicks.
:::



```{python}
# | code-fold: true

df_comparison = pl.concat(
    [
        df_output.with_columns(model=pl.lit("Simulated")),
        pl.from_pandas(df_fgoe)
        .with_columns(model=pl.lit("Traditional"))
        .select(["player_id", "player_name", "draw", "chain", "fgoe", "model"]),
    ]
)

df_comparison_r = pointrange_data(
    df_comparison, group=["player_id", "player_name", "model"]
).join(
    df_output.group_by(["player_name", "player_id"]).agg(
        plot_sort=pl.col("fgoe").mean(), simulations=pl.len()
    ),
    on=["player_id", "player_name"],
)

df_comparison_r = title_name(df_comparison_r)
(
    ggplot(
        df_comparison_r, aes("reorder(player_name, plot_sort)", "fgoe", color="model")
    )
    + geom_linerange(
        aes(ymin="hdi_lower", ymax="hdi_upper"), size=1.2, position=position_dodge(0.5)
    )
    + geom_pointrange(
        aes(ymin="ymin", ymax="ymax"),
        shape="o",
        fill="white",
        position=position_dodge(0.75),
    )
    + coord_flip()
    + labs(
        y="Simulated FGOE",
    )
    + theme_minimal()
    + theme(
        legend_position="top",
        axis_title_y=element_blank(),
        figure_size=(6, 8),
        dpi=100,
        panel_grid_minor=element_blank(),
        panel_grid_major_y=element_blank(),
        axis_text_y=element_text(size=8),
    )
    + geom_hline(yintercept=0, linetype="dashed")
    + labs(
        title="Simulated vs Traditional FGOE",
        y="Simulated FGOE",
        color="Model:",
    )
    + scale_color_manual(("#001e42", "#ff4b00"))
)
```


#### Which player would we prefer?

Now, even if a kicker hasn't attempted a 60 yard kick, we can provide an estimate for what that might look like.

```{python}
# | code-fold: true
plot_df = pl.from_pandas(
    df_sfgoe[df_sfgoe["player_name"].isin(["JUSTIN TUCKER", "ZANE GONZALEZ"])].assign(
        player_name=lambda x: x["player_name"].astype(str)
    )
)

(
    ggplot(aes("distance", "p_x", color="player_name"))
    + geom_ribbon(
        aes(y="fgoe", ymin="ymin", ymax="ymax", fill="player_name"),
        data=pointrange_data(plot_df, field="p_x", group=["player_name", "distance"]),
        alpha=0.2,
        outline_type=element_blank(),
        show_legend={"color": False},
    )
    + geom_ribbon(
        aes(y="fgoe", ymin="hdi_lower", ymax="hdi_upper", fill="player_name"),
        # fill="#00000053",
        data=pointrange_data(plot_df, field="p_x", group=["player_name", "distance"]),
        alpha=0.4,
        outline_type=element_blank(),
        show_legend={"color": False},
    )
    + geom_line(
        data=plot_df.group_by(["player_name", "distance"]).agg(pl.col("p_x").mean()),
        size=1.5,
        show_legend=False
    )
    + theme_minimal()
    + theme(legend_position="top", panel_grid_minor=element_blank())
    + labs(
        title="Modeled Probability by Distance",
        y="P(Make)",
        x="Distance",
        fill="Player:",
    )
    + scale_x_continuous(limits=(25, 60))
    + scale_color_manual(values=("#ff4b00", "#001e42"))
    + scale_fill_manual(values=("#ff4b00", "#001e42"))
)
```

## Leaderboard

The table below presents the generated leaderboard, showcasing the player_id, player_name, rating (Simulated FGOE), and rank for the top 10 and bottom 10 kickers evaluated by the model. The *rank* column is in ascending order, where a lower rank indicates a better performance. The *Top 5* column indicates the percentage of simulations in which a player ranked among the top 5 kickers, offering a probabilistic measure of their elite performance.

```{python}
# | code-fold: true


def create_leaderboard(df: pl.DataFrame) -> pl.DataFrame:
    """
    Create the leaderboard of FGOE values

    Args:
        df: (pl.DataFrame) dataframe from `create_fgoe_draws()`

    Returns:
        pl.DataFrame: Polars dataframe of leaderboard

    """
    idata_post = df.to_pandas().set_index(["chain", "draw", "player_name"]).to_xarray()

    # perc player was in top 5 of a draw
    df_top_5 = (
        df.with_columns(
            player_name=pl.col("player_name").cast(pl.Utf8),
            rank=pl.col("fgoe").rank("dense", descending=True).over(["draw", "chain"]),
        )
        .group_by(["player_name"])
        .agg(top_5=(pl.col("rank") <= 5).mean())
        .sort("top_5", descending=True)
    )

    # summary table of the metric, with rank added and joined with top 5
    leaderboard = (
        (
            pl.from_pandas(
                az.summary(idata_post, var_names=["fgoe"], hdi_prob=0.9)
                .reset_index()
                .assign(player_name=lambda x: x["index"].str.extract(r"fgoe\[(.+)\]"))
            )
            .join(df_top_5, on="player_name")
            .sort(["mean"], descending=True)
            .with_row_index("rank", offset=1)
            .rename({"mean": "rating"})
            .join(
                df[["player_name", "player_id"]]
                .unique()
                .with_columns(pl.col("player_name").cast(pl.Utf8)),
                on="player_name",
            )
        )
        .select(
            [
                "player_id",
                "player_name",
                "rank",
                "rating",
                "sd",
                "hdi_5%",
                "hdi_95%",
                "top_5",
            ]
        )
        .sort(["rank"])
    )

    return leaderboard


leaderboard = create_leaderboard(df_output)
leaderboard.write_csv("leaderboard.csv")

(
    GT(
        pl.concat([leaderboard.head(10), leaderboard.tail(10)]).rename(
            {"hdi_5%": "lower", "hdi_95%": "upper"}
        )
    )
    .tab_header(
        title="2018 NFL Kicker Leaderboard", subtitle="Top and bottom 10 players shown"
    )
    .tab_spanner(label="Simulated FGOE", columns=["rating", "sd", "lower", "upper"])
    # .tab_stubhead(label="rank")
    .fmt_number(columns=["rating", "sd", "lower", "upper"], decimals=2)
    .fmt_percent("top_5", decimals=1)
    .data_color(
        columns=["rating"],
        domain=[leaderboard["rating"].min(), leaderboard["rating"].max()],
        palette=["#535c78", "white", "#ff4b00"],
        na_color="white",
    )
    .data_color(
        columns=["lower"],
        domain=[leaderboard["hdi_5%"].min(), leaderboard["hdi_5%"].max()],
        palette=["#535c78", "white", "#ff4b00"],
        na_color="white",
    )
    .data_color(
        columns=["upper"],
        domain=[leaderboard["hdi_95%"].min(), leaderboard["hdi_95%"].max()],
        palette=["#535c78", "white", "#ff4b00"],
        na_color="white",
    )
    .data_color(
        columns=["top_5"],
        domain=[0, 1],
        palette=["white", "#ff4b00"],
        na_color="white",
    )
    .cols_label(
        player_id=html("Player Id"),
        player_name=html("Player Name"),
        rank="Rank",
        rating="Rating",
        sd="Variance",
        lower="Low-end",
        upper="High-end",
        top_5="Top 5",
    )
)

```

- **Justin Tucker** consistently ranks at the top of the leaderboard, demonstrating a high likelihood of being among the top 5 kickers by this metric. This positioning, even in seasons with a lot of variance (like 2018's blocked kicks), underscores the model's ability to provide a more stable and accurate assessment of true skill.
- **Robert Aguayo's** ranking reflects the influence of partial pooling from his prior seasons, contributing to his lower-end position despite a potentially stronger 2018 performance.
- **Tyler Davis** with only a single kick in the dataset, has a very wide credible interval. This indicates that while the model provides a point estimate, there is substantial uncertainty, suggesting his true skill could credibly range from one of the worst kickers to a top 5 performer. This demonstrates the model's ability to quantify uncertainty, allowing decision-makers to understand the confidence level in each player's evaluation, especially for those with sparse data.
