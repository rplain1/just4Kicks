---
engine: jupyter
---

## Executive Summary

Problem: Evaluating NFL kickers accurately mid-season presents a significant challenge due to the inherent sparsity of game data and the varying contexts of individual field goal attempts. Traditional metrics often struggle to provide insights into true player skill when sample sizes are limited.

Solution: This report proposes a Bayesian hierarchical model designed to evaluate the performance of NFL kickers. The model accounts for both individual player skill and the non-linear relationship between kick distance and success probability. By estimating unique player effects and then applying these predictions to a standardized set of simulated kicks (25-60 yards), we derive a contextualized and stable evaluation metric.

Key Findings: The Simulated Field Goal Over Expected (FGOE) metric, generated by this model, offers a superior and more reliable assessment of kicker performance compared to traditional observed metrics. This approach successfully mitigates the noise introduced by small sample sizes, enabling more confident comparisons across players. For example, our analysis demonstrates how the Simulated FGOE accurately positions elite kickers like Justin Tucker at the top, even when their traditional FGOE is skewed by anomalies (e.g., blocked kicks).

Impact: This model provides a quantifiable and nuanced understanding of kicker performance. It not only offers a clear "rating" but also quantifies the uncertainty surrounding each player's estimated skill through credible intervals. This allows decision-makers to distinguish between genuinely high-performing kickers and those whose performance might be inflated by favorable circumstances or limited data. Players with fewer historical kicks will naturally exhibit larger uncertainty bands, providing a realistic view of the confidence in their evaluation, while consistently strong performers will show tighter, higher-valued estimates. This data-driven insight empowers scouting teams and analysts to make more informed personnel decisions.

## Overview

- [Model Setup](#model-setup)
- [Traditional FGOE](#traditional-fgoe)
- [Simulated FGOE](#simulated-fgoe)
- [Leaderboard](#leaderboard)

Assumptions:

- This represents the initial request. The focus is to start with an effective but simple approach, then build iteratively with support from scouting teams and other functional areas within the organization.
- Explainability is prioritized.

Library imports

```{python}
#| code-fold: true
import bambi as bmb
import polars as pl
import numpy as np
import arviz as az
import polars as pl
import bambi as bmb
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# functions for data wrangling and plotting
from utils import (
    create_grid,
    create_leaderboard,
    plot_fgoe,
    prep_data,
)

from great_tables import GT, md, html

```


### Data

```{python}

df = prep_data()
# Bambi currently expects Pandas DataFrames, so the Polars DataFrame is converted for model fitting.
df_model_pd = df.filter(pl.col('season_type') == 'Reg').to_pandas()
df.glimpse()
```

Players have varying skill levels that traditional analytics cannot fully capture. The sparse data and hierarchical structure of the underlying process make this well-suited for hierarchical modeling approaches.

### Model Setup

The following model was used:


\begin{aligned}
y_i &\sim \operatorname{Bernoulli}(p_i) \\
\operatorname{logit}(p_i) &= \alpha_{j[i]} + \sum_{k=1}^K B_k(x_i) \, \beta_k \\
\\
\alpha_j &\sim \operatorname{Student-t}(3, 0, \sigma_\alpha) \\
\sigma_\alpha &\sim \operatorname{HalfNormal}(0.5) \\
\\
\beta_k &\sim \operatorname{Student-t}(3, 0, 1.5) \quad \text{for } k = 1, \dots, K \\
\\
\mu_\text{global} &\sim \mathcal{N}(0, 1.5)
\end{aligned}

- Bernoulli likelihood for the binary outcome
- Partially pooled intercepts by player
- The distribution of player effects share the same sigma parameter
- Spline with basis functions applied to distance


Student-t priors were chosen for their heavier tails, providing more robust estimates to handle outliers or less certain prior knowledge, particularly for individual player effects.

```{python}
# spline specification, miss you mgcv::s()
s = "bs(distance, df=4, intercept=False)"
priors = {
    # priors on spline
    s: bmb.Prior("StudentT", nu=3, mu=0, sigma=1.5),
    # prior for player componenent, shared sigma parameter
    "1|player_id": bmb.Prior(
        "StudentT", nu=3, mu=0, sigma=bmb.Prior("HalfNormal", sigma=0.5)
    ),
}

# straightforward bambi definition
model = bmb.Model(
    f"success ~ (1|player_id) + {s}",
    data=df_model_pd,
    family="bernoulli",
    priors=priors,
    # these are default but critical to understand
    center_predictors=True, # better numerical stability
    noncentered=True # non-centered parameterization helps with hard to sample geometries
)

model

```

This model generates player-specific intercepts that adapt based on individual performance and data availability. Players with more observations will have their estimates pulled toward their own performance, while those with limited data will remain closer to the population prior. The shared variance across all players creates partial pooling, where information is borrowed between players to improve estimates for those with limited data.

```{python}
# spline caused issue with multiprocessing, too much to debug for a weekend
idata = model.fit(draws=1000, chains=4, cores=1)
az.plot_trace(idata)
```

It's important to check diagnostics in Bayesian models. While this doesn't account for causality or domain-specific modeling checks, it ensures the MCMC algorithm is working as expected. All chains converged without divergent transitions, and effective sample sizes are adequate.

#### Spline

There is evidence to suggest a non-linear relationship between distance and success rate. Using splines, the model estimates this relationship much more realistically than a polynomial model would.

```{python}
#| code-fold: true
df_plot = df.group_by("distance").agg(pl.col("success").sum() / pl.count("success"))
sns.regplot(x="distance", y="success", data=df_plot, lowess=True)
plt.title("Success Rate by Distance - Observed Data")
plt.xlabel("Attempt Distance (yards)", fontsize=14)
plt.ylabel("Success Rate", fontsize=14)
```

The table below shows the summary of spline coefficient distributions.

```{python}
#| code-fold: true
az.summary(idata, var_names=["Intercept", s])
```

Unlike player effect coefficients, they are not as easily interpretable in a table. The plots below are to help understand how they come together. First, we have each basis function and it's new shape after learning parameter weights.

```{python}
#| code-fold: true
idata.posterior[s].to_dataframe()

w = idata.posterior[s].mean(("chain", "draw")).values

spline_basis = pd.DataFrame(model.components["p"].design.common[s])

df_spline = (spline_basis * w.T).assign(distance = df_model_pd['distance']).melt("distance", var_name="basis_idx", value_name="value").assign(basis_idx = lambda x: x['basis_idx'].astype(str))#.query('distance > 20')
```

Each basis function with the model estimates.

```{python}

sns.lineplot(df_spline, x="distance", y="value", hue="basis_idx")
plt.title("Basis function's shape")
```

Summing them together produces the estimated effect of distance on FG success.

```{python}
sns.lineplot(df_spline.groupby("distance")["value"].mean())
plt.title("General spline shape")
```

```{python}
az.summary(idata, var_names='1|player_id').sort_values('mean', ascending=False)
```

## Field Goals Over Expected

### Traditional FGOE

What I outlined in my application was establishing at minimum a Field Goal Over Expected model. This contextualizes each kick and helps identify players over- or underperforming relative to expectation. This metric is used across multiple sports because it's intuitive and easily digestible, especially for audiences with less analytical training.

#### 2018 results

```{python}
df_2018 = df_model_pd[df_model_pd["season"] == 2018]

df_goe = model.predict(idata, data=df_2018, inplace=False)

# takes the posterior predictive distribution and subtracts the
# predicted probability from the observed kicks
idata_fgoe = (
    df_goe["posterior"]["p"]
    .to_dataframe()
    .reset_index()
    .merge(df_2018.assign(__obs__=lambda x: np.arange(x.shape[0])), on="__obs__")
    .assign(fgoe=lambda x: x["success"] - x["p"])[
        ["chain", "draw", "fgoe", "player_name", "__obs__"]
    ]
    .groupby(["chain", "draw", "player_name"], observed=True)
    .agg({"fgoe": "sum"})
    .to_xarray()
)

plot_fgoe(idata_fgoe, "Field Goal Over Expected (FGOE) by Player - Blocks Removed")
```

Justin Tucker is the 20th best kicker? Pitching this model in 2018 would have been difficult, and rightfully so.

The problem with using this metric alone is that limited sample sizes can introduce significant noise. Upon examining the dataset, there was a blocked field goal against Tucker. While outside the scope of this analysis, it's uncertain how much credit should be assigned to the kicker versus the blocking. When blocked kicks are removed, the model results align more closely with expectations.

#### 2018 results - w/out blocks

```{python}
#| code-fold: true
idata_fgoe_no_block = (
    df_goe["posterior"]["p"]
    .to_dataframe()
    .reset_index()
    .merge(
        df_2018[~df_2018["field_goal_result"].isin(["Blocked"])].assign(
            __obs__=lambda x: np.arange(x.shape[0])
        ),
        on="__obs__",
    )
    .assign(fgoe=lambda x: x["success"] - x["p"])[
        ["chain", "draw", "fgoe", "player_name", "__obs__"]
    ]
    .groupby(["chain", "draw", "player_name"], observed=True)
    .agg({"fgoe": "sum"})
    .to_xarray()
)

plot_fgoe(
    idata_fgoe_no_block, "Field Goal Over Expected (FGOE) by Player - Blocks Removed"
)

```

However, that approach is arbitrary, and a more robust solution is needed.

---

#### Simulated FGOE

The model allows us to extract each player's intercept parameter after accounting for kick distance, and apply it to simulated data. This enables what-if scenarios and projections beyond only observed data. By modeling all kickers on the same set of kicks, we can establish expectations for what each kicker should make.

I chose linear intervals for kicks from 25-60 yards. This approach prioritizes simplicity â€” it's easy to explain and represents the first step toward gaining organizational buy-in for the model.


```{python}
# This creates a standardized grid of kick distances for all players in 2018 to simulate performance under consistent conditions
df_grid_18 = create_grid(df_model_pd)
```

```{python}
# Get posterior predictions without group effects
avg_preds = model.predict(
    idata, data=df_grid_18, inplace=False, include_group_specific=False
)

# posterior predictions on grid
preds = model.predict(idata, data=df_grid_18, inplace=False)

df_pred = preds.posterior["p"].to_dataframe().reset_index()
df_avg_pred = avg_preds["posterior"]["p"].to_dataframe().reset_index()

# take the predictions form the players and subtract the posterior predictions without group effects. This provides the main source of the metric for simulated FGOE
df_fgoe = (
    df_pred.merge(df_avg_pred, on=["chain", "draw", "__obs__"], how="inner")
    .merge(
        df_grid_18[["player_id", "player_name", "distance", "__obs__"]],
        on=["__obs__"],
        how="left",
    )
    .assign(fgoe=lambda x: x["p_x"] - x["p_y"])
)

# polars makes dataframe go brrr
df_output = (
    pl.from_pandas(df_fgoe)
    .group_by(["player_id", "player_name", "draw", "chain"])
    .agg(pl.col("fgoe").sum())
)


```

With the simulation of kicks, we can now look at a *Simulated* FGOE. This operates at the scale of posterior sample draws, with each player having 4,000 simulations across a distribution of kicks. This approach provides the ability to quantify uncertainty within our estimates and examine the range of potential outcomes. Additionally addresses the sample size problem of a traditional FGOE.


```{python}
idata_post = df_output.to_pandas().set_index(['chain', 'draw', 'player_name']).to_xarray()


plot_fgoe(idata_post, "Simulated FGOE")
```

To compare the traditional vs simulated FGOE, we can look at the posterior distributions between the two approachs.

::: {.callout-note}
They aren't necesarily 1:1 comparisons of the same amount of kicks, because in the traditional version we are limited to the kicks that the kicker attempted. What is more important is to look directionally at what the simulated model does compared to the traditional. Easily shown is Justin Tucker is firmly at the top of the list, without having to adjust arbitrarily for blocked kicks.
:::

```{python}

fgoe_mean = idata_post["fgoe"].mean(("chain", "draw")) * -1

axs = az.plot_forest(
    [idata_post.sortby(fgoe_mean), idata_fgoe.sortby(fgoe_mean)],
    var_names=["fgoe"],
    model_names=["Simulated", "Traditional"],
    combined=True,
)
ax = axs[0]
ax.axvline(x=0, color="black", linestyle="--", linewidth=1)
ax.set_title("Comparison")
ax.set_xlabel("FGOE")
ax.tick_params(axis="y", labelsize=8)
```

Now, even if a kicker hasn't attempted a 60 yard kick, we can provide an estimate for what that might look like.

```{python}
plot_df = df_fgoe[
    df_fgoe["player_name"].isin(["JUSTIN TUCKER", "ZANE GONZALEZ"])
].assign(player_name=lambda x: x["player_name"].astype(str))

sns.lineplot(plot_df, x="distance", y="p_x", hue="player_name", errorbar=("pi", 90))
plt.title("Comparison of kicker by distance")
plt.xlabel("Attempt Distance (yards)", fontsize=14)
plt.ylabel("P(Make)", fontsize=14)
```

## Leaderboard

The leaderboard contains:

```{python}
#| code-fold: true
leaderboard = create_leaderboard(df_output, idata_post)

(
    GT(
        pl.concat([leaderboard.head(10), leaderboard.tail(10)])
        .rename({'hdi_5%': 'lower', 'hdi_95%': 'upper'}))
    .tab_header(title="2018 NFL Kicker Leaderboard")
    .tab_spanner(label="Simulated FGOE", columns=["rating", "sd", "lower", "upper"])
    #.tab_stubhead(label="rank")
    .fmt_number(columns=["rating", "sd", "lower", "upper"], decimals=2)
    .fmt_percent("top_5", decimals=1)
    .data_color(
        columns=["rating"],
        domain=[leaderboard["rating"].min(), leaderboard["rating"].max()],
        palette=["rebeccapurple", "white", "orange"],
        na_color="white",
    )
    .data_color(
        columns=["lower"],
        domain=[leaderboard["hdi_5%"].min(), leaderboard["hdi_5%"].max()],
        palette=["rebeccapurple", "white", "orange"],
        na_color="white",
    )
    .data_color(
        columns=["upper"],
        domain=[leaderboard["hdi_95%"].min(), leaderboard["hdi_95%"].max()],
        palette=["rebeccapurple", "white", "orange"],
        na_color="white",
    )
    .cols_label(
        player_id=html("Player Id"),
        player_name=html("Player Name"),
        rank = "Rank",
        rating = 'Rating',
        sd = 'Variance',
        lower = 'Low-end',
        upper = 'High-end',
        top_5 = 'Top 5'
    )
)

```
